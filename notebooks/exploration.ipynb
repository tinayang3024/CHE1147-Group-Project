{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Data Load & Basic Cleaning] shape: (178144, 20)\n",
            "[Temperature (to °C) and pH parsing] shape: (178144, 23)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sequence feats: 100%|██████████| 178144/178144 [00:33<00:00, 5279.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sequence feature computation] shape: (178144, 53)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 248\u001b[0m\n\u001b[0;32m    246\u001b[0m enzy_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m enzy_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmiles\u001b[39m\u001b[38;5;124m\"\u001b[39m, pd\u001b[38;5;241m.\u001b[39mSeries(index\u001b[38;5;241m=\u001b[39menzy_data\u001b[38;5;241m.\u001b[39mindex))\u001b[38;5;241m.\u001b[39mapply(smiles_to_mol)\n\u001b[0;32m    247\u001b[0m desc_df \u001b[38;5;241m=\u001b[39m enzy_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(mol_descriptors)\u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 248\u001b[0m fps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(enzy_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(mol_fingerprint))\n\u001b[0;32m    249\u001b[0m fp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(fps, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# add log scaling for MW, TFSA due to right skew\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
            "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
            "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
            "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[1], line 242\u001b[0m, in \u001b[0;36mmol_fingerprint\u001b[1;34m(mol, generator, nbits)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(nbits, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    241\u001b[0m fp \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mGetFingerprint(mol)\n\u001b[1;32m--> 242\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nbits,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    243\u001b[0m DataStructs\u001b[38;5;241m.\u001b[39mConvertToNumpyArray(fp, arr)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Enzyme EDA + Feature Engineering (Clean Script)\n",
        "# --------------------------------------------\n",
        "# Requirements (install once in your env):\n",
        "#   pip install pandas numpy matplotlib seaborn pyarrow requests rdkit-pypi tqdm scipy statsmodels biopython scikit-learn\n",
        "# ============================================\n",
        "# ---------- Imports ----------\n",
        "import os, io, re, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, DataStructs, rdFingerprintGenerator\n",
        "from rdkit import RDLogger\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from functools import reduce\n",
        "from typing import Optional\n",
        "import umap\n",
        "\n",
        "# ---------- Config ----------\n",
        "FIG_DIR = \"./figs\"; os.makedirs(FIG_DIR, exist_ok=True)\n",
        "DATA_URL = \"https://github.com/ChemBioHTP/EnzyExtract/raw/main/data/export/TheData_kcat.parquet\"\n",
        "\n",
        "# Feature-reduction params\n",
        "CORR_THRESHOLD = 0.90        # drop one of a pair if |corr| > 0.90 (continuous only)\n",
        "FP_VAR_THRESHOLD = 0.01      # drop fingerprint bits with variance <= 0.01\n",
        "USE_PCA_FOR_FP = True        # compress FP after variance filtering\n",
        "FP_PCA_N_COMPONENTS = 200\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "TARGET_COLS = [\"kcat_km\", \"kcat_value\", \"km_value\"]  # excluded from feature transforms\n",
        "\n",
        "# ======================================================\n",
        "# 1) Load & basic cleaning\n",
        "# ======================================================\n",
        "def load_data(url: str) -> pd.DataFrame:\n",
        "    resp = requests.get(url); resp.raise_for_status()\n",
        "    return pq.read_table(io.BytesIO(resp.content)).to_pandas()\n",
        "\n",
        "enzy_data = load_data(DATA_URL)\n",
        "# Keep parsed numeric columns; drop raw text duplicates\n",
        "enzy_data = enzy_data.drop(columns=[c for c in [\"kcat\", \"km\"] if c in enzy_data.columns])\n",
        "# Keep rows where kcat_value and km_value are defined\n",
        "enzy_data = enzy_data.dropna(subset=[\"kcat_value\", \"km_value\"]).reset_index(drop=True)\n",
        "\n",
        "# Drop columns with >50% missing (simple, defensible rule)\n",
        "missing_ratio = enzy_data.isnull().mean()\n",
        "enzy_data = enzy_data.drop(columns=missing_ratio[missing_ratio > 0.5].index)\n",
        "#Drop \"Similariity\" over 60%\n",
        "enzy_data = enzy_data[(enzy_data[\"max_enzyme_similarity\"].isna()) | (enzy_data[\"max_enzyme_similarity\"] >= 60)]\n",
        "enzy_data = enzy_data[(enzy_data[\"max_organism_similarity\"].isna()) | (enzy_data[\"max_organism_similarity\"] >= 60)]\n",
        "\n",
        "print(\"[Data Load & Basic Cleaning] shape:\", enzy_data.shape)\n",
        "\n",
        "# ======================================================\n",
        "# 2) Temperature parsing (to °C) and pH parsing\n",
        "# ======================================================\n",
        "def extract_numeric_segment(text: str):\n",
        "    \"\"\"Return (numeric_part, had_unit). Keeps '±' part, ignores trailing unit text.\"\"\"\n",
        "    if not text: return \"\", False\n",
        "    t = str(text).strip().replace(\"×\", \"x\").replace(\"X\", \"x\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    m = re.match(r\"^\\s*([\\d\\.\\+\\-eExx^ ]+(?:\\s*(?:±|[-–—to])\\s*[\\d\\.\\+\\-eExx^ ]+)?)\\s*(.*)$\", t, re.IGNORECASE)\n",
        "    if not m: return t, False\n",
        "    return m.group(1).strip(), bool(m.group(2).strip())\n",
        "\n",
        "def _convert_token(tok: str):\n",
        "    \"\"\"Convert '1.3e4', '2x10^3' → float; else NaN.\"\"\"\n",
        "    if not tok: return np.nan\n",
        "    t = str(tok).strip().lower().replace(\"×\", \"x\").replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"−\", \"-\")\n",
        "    try: return float(t)\n",
        "    except ValueError: pass\n",
        "    m = re.match(r\"([\\+\\-]?\\d*\\.?\\d+)\\s*[x]\\s*10\\^?([\\+\\-]?\\d+)\", t)\n",
        "    return float(m.group(1)) * (10 ** int(m.group(2))) if m else np.nan\n",
        "\n",
        "def parse_numeric_or_range(s: str):\n",
        "    \"\"\"Return midpoint for ranges, single value for ±, NaN if fail.\"\"\"\n",
        "    if not s: return np.nan\n",
        "    t = str(s)\n",
        "    t = t.replace(\"×\",\"x\").replace(\"–\",\"-\").replace(\"—\",\"-\").replace(\"−\",\"-\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t); t = re.sub(r\"\\bto\\b\", \"-\", t, flags=re.I)\n",
        "    t = re.sub(r\"\\s*±\\s*[\\d\\.eE\\+\\-]+\", \"\", t)  # drop ± error\n",
        "    tokens = re.findall(r\"([\\+\\-]?\\d*\\.?\\d+(?:[eE][\\+\\-]?\\d+)?|[\\+\\-]?\\d*\\.?\\d+\\s*x\\s*10\\^?[\\+\\-]?\\d+)\", t)\n",
        "    if not tokens: return np.nan\n",
        "    if len(tokens) == 1: return _convert_token(tokens[0])\n",
        "    lo, hi = _convert_token(tokens[0]), _convert_token(tokens[1])\n",
        "    return (lo + hi) / 2.0 if np.isfinite(lo) and np.isfinite(hi) else np.nan\n",
        "\n",
        "def _detect_temp_unit(text: str):\n",
        "    \"\"\"Return ('C'|'K'|'F'|None, factor, offset) to convert to °C.\"\"\"\n",
        "    if re.search(r\"[°\\s]*c\\b\", text, re.I): return \"C\", 1.0, 0.0\n",
        "    if re.search(r\"[°\\s]*k\\b\", text, re.I): return \"K\", 1.0, -273.15\n",
        "    if re.search(r\"[°\\s]*f\\b\", text, re.I): return \"F\", 5/9, -32 * 5/9\n",
        "    return None, None, None\n",
        "\n",
        "def preprocess_temperature_field(text):\n",
        "    \"\"\"Remove pH snippets; map 'room temperature' to ~22°C unless explicit number given.\"\"\"\n",
        "    if pd.isna(text): return np.nan\n",
        "    t = str(text).strip().lower()\n",
        "    t = re.sub(r\"ph\\s*\\d+(\\.\\d+)?\\s*,?\\s*\", \"\", t)\n",
        "    if \"room temperature\" in t or \"ambient temperature\" in t or re.search(r\"\\brt\\b\", t):\n",
        "        m = re.search(r\"(\\d+(\\.\\d+)?)\\s*°?\\s*c\", t)\n",
        "        return (m.group(1) + \"°C\") if m else \"22°C\"\n",
        "    if \"not mentioned\" in t: return np.nan\n",
        "    return t.strip(\" ,;()[]\")\n",
        "\n",
        "temp_counts = {\"valid\": 0, \"unavailable\": 0, \"failed\": 0, \"unrecognized\": 0}\n",
        "\n",
        "def clean_temperature(value):\n",
        "    \"\"\"Parse '37 °C', '310 K', '30 ± 2°C', '20-25 °C' → °C (float).\"\"\"\n",
        "    if pd.isna(value) or str(value).strip()==\"\":\n",
        "        temp_counts[\"unavailable\"] += 1; return np.nan\n",
        "    text = str(value)\n",
        "    numeric_seg, had_unit = extract_numeric_segment(text)\n",
        "    val = parse_numeric_or_range(numeric_seg)\n",
        "    if not np.isfinite(val): temp_counts[\"failed\"] += 1; return np.nan\n",
        "    unit, factor, offset = _detect_temp_unit(text)\n",
        "    if unit == \"C\": temp_counts[\"valid\"] += 1; return val\n",
        "    if unit == \"K\": temp_counts[\"valid\"] += 1; return val - 273.15\n",
        "    if unit == \"F\": temp_counts[\"valid\"] += 1; return (val - 32) * 5/9\n",
        "    if not had_unit: return val  # assume °C by convention\n",
        "    temp_counts[\"unrecognized\"] += 1; return np.nan\n",
        "\n",
        "enzy_data[\"temperature_pre\"] = enzy_data.get(\"temperature\", pd.Series(index=enzy_data.index)).apply(preprocess_temperature_field)\n",
        "enzy_data[\"temperature_C\"] = enzy_data[\"temperature_pre\"].apply(clean_temperature)\n",
        "\n",
        "# pH cleaner (simple numeric/range + a few qualitative words)\n",
        "ph_counts = {\"valid\": 0, \"failed\": 0, \"unavailable\": 0, \"unrecognized\": 0}\n",
        "\n",
        "def clean_ph(value):\n",
        "    if pd.isna(value) or str(value).strip()==\"\":\n",
        "        ph_counts[\"unavailable\"] += 1; return np.nan\n",
        "    t = str(value).strip().lower()\n",
        "    if any(x in t for x in [\"unknown\", \"n/a\", \"—\", \"–\"]):\n",
        "        ph_counts[\"unavailable\"] += 1; return np.nan\n",
        "    if \"neutral\" in t: return 7.0\n",
        "    if \"acidic\" in t:  return 5.0\n",
        "    if \"basic\"  in t or \"alkaline\" in t: return 9.0\n",
        "    m = re.match(r\"p?h?\\s*([\\d\\.]+)\\s*[-–to]+\\s*([\\d\\.]+)\", t)\n",
        "    if m:\n",
        "        try: low, high = float(m.group(1)), float(m.group(2)); ph_counts[\"valid\"] += 1; return (low + high)/2\n",
        "        except: ph_counts[\"failed\"] += 1; return np.nan\n",
        "    m = re.match(r\"p?h?\\s*([\\d\\.]+)\", t)\n",
        "    if m:\n",
        "        try: v = float(m.group(1)); ph_counts[\"valid\"] += 1; return v\n",
        "        except: ph_counts[\"failed\"] += 1; return np.nan\n",
        "    ph_counts[\"unrecognized\"] += 1; return np.nan\n",
        "\n",
        "enzy_data[\"pH_value\"] = enzy_data.get(\"pH\", pd.Series(index=enzy_data.index)).apply(clean_ph)\n",
        "print(\"[Temperature (to °C) and pH parsing] shape:\", enzy_data.shape)\n",
        "# ======================================================\n",
        "# 3) Sequence features (ProteinAnalysis) with robust cleaning\n",
        "# ======================================================\n",
        "STD = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "REPLACEMENTS = {\n",
        "    \"U\": \"C\",  # selenocysteine -> cysteine proxy\n",
        "    \"O\": \"K\",  # pyrrolysine -> lysine proxy\n",
        "    \"B\": \"D\",  # D/N ambiguity -> aspartate\n",
        "    \"Z\": \"E\",  # E/Q ambiguity -> glutamate\n",
        "    \"J\": \"L\",  # I/L ambiguity -> leucine\n",
        "}\n",
        "paren_pat = re.compile(r\"\\([^)]*\\)\")\n",
        "\n",
        "def normalize_sequence(seq: str):\n",
        "    s = str(seq)\n",
        "    s = paren_pat.sub(\"\", s)\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    s = re.sub(r\"[^A-Za-z]\", \"\", s).upper()\n",
        "    for k,v in REPLACEMENTS.items(): s = s.replace(k, v)\n",
        "    orig_len = len(s)\n",
        "    s_clean = \"\".join(ch for ch in s if ch in STD)\n",
        "    unknown_frac = (1 - len(s_clean)/orig_len) if orig_len else np.nan\n",
        "    return s_clean, unknown_frac, orig_len\n",
        "\n",
        "def compute_protein_features(seq):\n",
        "    s_clean, unknown_frac, orig_len = normalize_sequence(seq)\n",
        "    if not s_clean or orig_len < 10:\n",
        "        return {}\n",
        "    pa = ProteinAnalysis(s_clean)\n",
        "    feats = {\n",
        "        \"sequence_length\": orig_len,\n",
        "        \"clean_length\": len(s_clean),\n",
        "        \"unknown_frac\": unknown_frac,\n",
        "        \"aromaticity\": pa.aromaticity(),\n",
        "        \"instability_index\": pa.instability_index(),\n",
        "        \"isoelectric_point\": pa.isoelectric_point(),\n",
        "        \"gravy\": pa.gravy(),\n",
        "    }\n",
        "    h, t, e = pa.secondary_structure_fraction()\n",
        "    feats.update({\"frac_helix\": h, \"frac_turn\": t, \"frac_sheet\": e})\n",
        "\n",
        "    # was: pa.get_amino_acids_percent()  -> deprecated\n",
        "    for aa, frac in pa.amino_acids_percent.items():\n",
        "        feats[f\"aa_{aa}\"] = frac\n",
        "    return feats\n",
        "\n",
        "seq_feats = [compute_protein_features(s) for s in tqdm(enzy_data.get(\"sequence\", pd.Series(index=enzy_data.index)).astype(str), desc=\"Sequence feats\")]\n",
        "enzy_data = pd.concat([enzy_data.reset_index(drop=True), pd.DataFrame(seq_feats).reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(\"[Sequence feature computation] shape:\", enzy_data.shape)\n",
        "\n",
        "# ======================================================\n",
        "# 4) SMILES → RDKit descriptors + Morgan fingerprints\n",
        "# ======================================================\n",
        "RDLogger.DisableLog('rdApp.*')         # suppress RDKit warnings\n",
        "FP_SIZE = 2048\n",
        "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=FP_SIZE)\n",
        "\n",
        "def smiles_to_mol(s): \n",
        "    try: return Chem.MolFromSmiles(s, sanitize=True)\n",
        "    except: return None\n",
        "\n",
        "def mol_descriptors(mol):\n",
        "    if mol is None:\n",
        "        return pd.Series({\"MW\": np.nan, \"LogP\": np.nan, \"NumHDonors\": np.nan, \"NumHAcceptors\": np.nan,\n",
        "                          \"TPSA\": np.nan, \"RotBonds\": np.nan, \"AromaticRingCount\": np.nan, \"RingCount\": np.nan})\n",
        "    return pd.Series({\n",
        "        \"MW\": Descriptors.MolWt(mol),\n",
        "        \"LogP\": Descriptors.MolLogP(mol),\n",
        "        \"NumHDonors\": Descriptors.NumHDonors(mol),\n",
        "        \"NumHAcceptors\": Descriptors.NumHAcceptors(mol),\n",
        "        \"TPSA\": Descriptors.TPSA(mol),\n",
        "        \"RotBonds\": Descriptors.NumRotatableBonds(mol),\n",
        "        \"AromaticRingCount\": Descriptors.NumAromaticRings(mol),\n",
        "        \"RingCount\": Descriptors.RingCount(mol),\n",
        "    })\n",
        "\n",
        "def mol_fingerprint(mol, generator=morgan_gen, nbits=FP_SIZE):\n",
        "    if mol is None: return np.zeros(nbits, dtype=int)\n",
        "    fp = generator.GetFingerprint(mol)\n",
        "    arr = np.zeros((nbits,), dtype=int)\n",
        "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    return arr\n",
        "\n",
        "enzy_data[\"mol\"] = enzy_data.get(\"smiles\", pd.Series(index=enzy_data.index)).apply(smiles_to_mol)\n",
        "desc_df = enzy_data[\"mol\"].apply(mol_descriptors).add_prefix(\"desc_\")\n",
        "fps = np.vstack(enzy_data[\"mol\"].apply(mol_fingerprint))\n",
        "fp_df = pd.DataFrame(fps, columns=[f\"fp_{i}\" for i in range(fps.shape[1])])\n",
        "\n",
        "# add log scaling for MW, TFSA due to right skew\n",
        "desc_df[\"desc_log_MW\"] = np.log10(desc_df[\"desc_MW\"])\n",
        "desc_df[\"desc_log_TPSA\"] = np.log1p(desc_df[\"desc_TPSA\"])\n",
        "\n",
        "# drop original mol column\n",
        "desc_df = desc_df.drop(columns=[\"desc_MW\", \"desc_TPSA\",])\n",
        "\n",
        "enzy_data = pd.concat([enzy_data.reset_index(drop=True), desc_df.reset_index(drop=True), fp_df], axis=1)\n",
        "\n",
        "print(\"[Molecule descriptors and fingerprints] shape:\", enzy_data.shape)\n",
        "\n",
        "# ======================================================\n",
        "# 5) Feature pruning (corr on continuous; variance + PCA on fingerprints)\n",
        "# ======================================================\n",
        "def split_feature_blocks(df: pd.DataFrame):\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    num_cols = [c for c in num_cols if c not in TARGET_COLS]\n",
        "    fp_cols = [c for c in num_cols if c.startswith(\"fp_\")]\n",
        "    cont_cols = [c for c in num_cols if c not in fp_cols]\n",
        "    return cont_cols, fp_cols\n",
        "\n",
        "def correlation_prune(df: pd.DataFrame, cols: list, thr: float):\n",
        "    if not cols: return []\n",
        "    corr = df[cols].corr().abs()\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    to_drop = [c for c in upper.columns if any(upper[c] > thr)]\n",
        "    kept = [c for c in cols if c not in to_drop]\n",
        "    print(f\"[Correlation prune] {len(cols)} → keep {len(kept)} (thr={thr})\")\n",
        "    return kept\n",
        "\n",
        "def reduce_fps(df: pd.DataFrame, fp_cols: list, var_thr=0.01, use_pca=False, pca_n=256, rs=42):\n",
        "    if not fp_cols: \n",
        "        return np.empty((len(df), 0)), [], {\"vt\": None, \"pca\": None}\n",
        "    X = df[fp_cols].fillna(0).astype(np.float32).values\n",
        "    vt = VarianceThreshold(threshold=var_thr)\n",
        "    Xr = vt.fit_transform(X)\n",
        "    kept_mask = vt.get_support()\n",
        "    kept_cols = [c for c, keep in zip(fp_cols, kept_mask) if keep]\n",
        "    print(f\"[FP variance] {len(fp_cols)} → keep {Xr.shape[1]} (thr={var_thr})\")\n",
        "    pca_model, names = None, kept_cols\n",
        "    if use_pca and Xr.shape[1] > 0:\n",
        "        print(f\"[FP PCA] applying PCA to reduce to {pca_n} components\")\n",
        "        pca_model = PCA(n_components=min(pca_n, Xr.shape[1]), random_state=rs)\n",
        "        Xr = pca_model.fit_transform(Xr)\n",
        "        names = [f\"fp_pca_{i}\" for i in range(Xr.shape[1])]\n",
        "        print(f\"[FP PCA] reduced to {Xr.shape[1]} comps\")\n",
        "    return Xr, names, {\"vt\": vt, \"pca\": pca_model, \"mask\": kept_mask}\n",
        "\n",
        "def prepare_features(df_in: pd.DataFrame):\n",
        "    df = df_in.copy()\n",
        "    y = {t: df[t].values if t in df.columns else None for t in TARGET_COLS}\n",
        "    cont_cols, fp_cols = split_feature_blocks(df)\n",
        "    cont_keep = correlation_prune(df, cont_cols, CORR_THRESHOLD)\n",
        "\n",
        "    cont_imp = SimpleImputer(strategy=\"median\")\n",
        "    cont_scaler = StandardScaler()\n",
        "    X_cont = cont_scaler.fit_transform(cont_imp.fit_transform(df[cont_keep])) if cont_keep else np.empty((len(df), 0))\n",
        "\n",
        "    X_fp, fp_names, fp_steps = reduce_fps(df, fp_cols, FP_VAR_THRESHOLD, USE_PCA_FOR_FP, FP_PCA_N_COMPONENTS, RANDOM_STATE)\n",
        "    X = np.hstack([X_cont, X_fp]) if X_fp.size else X_cont\n",
        "    meta = {\n",
        "        \"cont_cols_in\": cont_cols, \"cont_cols_kept\": cont_keep,\n",
        "        \"fp_cols_in\": fp_cols, \"fp_cols_kept\": fp_names,\n",
        "        \"imputer\": cont_imp, \"scaler\": cont_scaler,\n",
        "        \"fp_var_selector\": fp_steps[\"vt\"], \"fp_pca\": fp_steps[\"pca\"], \"fp_mask\": fp_steps.get(\"mask\"),\n",
        "    }\n",
        "    print(f\"[Final features] fp_pca: {meta['fp_pca'] is not None}\")\n",
        "    print(f\"[Final features] X shape: {X.shape}\")\n",
        "    return X, y, meta\n",
        "\n",
        "print(\"[Feature pruning] shape:\", enzy_data.shape)\n",
        "\n",
        "# ======================================================\n",
        "# 6) Plots for reporting (pre/post corr; FP variance; PCA scree)\n",
        "# ======================================================\n",
        "def plot_corr_heatmap(df, cols, title, fname, sample_max=5000):\n",
        "    if not cols: return\n",
        "    sub = df[cols]\n",
        "    if len(sub) > sample_max: sub = sub.sample(sample_max, random_state=42)\n",
        "    corr = sub.corr()\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", vmin=-1, vmax=1, xticklabels=False, yticklabels=False)\n",
        "    plt.title(title); plt.tight_layout()\n",
        "    out = os.path.join(FIG_DIR, fname); plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "def plot_pca_explained_variance(pca_model, fname=\"fp_pca_explained_variance.png\"):\n",
        "    \"\"\"\n",
        "    Scree plot for PCA on fingerprints with legend and 95% variance threshold line.\n",
        "    \"\"\"\n",
        "    if pca_model is None:\n",
        "        return\n",
        "\n",
        "    evr = pca_model.explained_variance_ratio_\n",
        "    cum = np.cumsum(evr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(range(1, len(evr)+1), evr, marker=\"o\", label=\"Per-component variance\")\n",
        "    plt.plot(range(1, len(evr)+1), cum, marker=\".\", label=\"Cumulative variance\")\n",
        "\n",
        "    # Add horizontal line at 0.95 cumulative explained variance\n",
        "    plt.axhline(y=0.95, color=\"red\", linestyle=\"--\", linewidth=1, label=\"95% variance threshold\")\n",
        "\n",
        "    plt.xlabel(\"PCA component\")\n",
        "    plt.ylabel(\"Explained variance (ratio)\")\n",
        "    plt.title(\"FP PCA: explained variance (per-comp and cumulative)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[saved] {out}\")\n",
        "\n",
        "\n",
        "def plot_fp_variance_and_kept(fps_df, kept_mask, title_prefix=\"FP\",\n",
        "                              var_threshold=0.01, fname_prefix=\"fp\"):\n",
        "    \"\"\"\n",
        "    Histogram of fingerprint bit variances + bar for kept vs dropped.\n",
        "    \"\"\"\n",
        "    if fps_df.empty:\n",
        "        return\n",
        "    X = fps_df.fillna(0).astype(np.float32).values\n",
        "    vars_ = X.var(axis=0)\n",
        "\n",
        "    # 1) Variance histogram\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(vars_, bins=50)\n",
        "    plt.axvline(var_threshold, linestyle=\"--\")\n",
        "    plt.title(f\"{title_prefix}: Bit Variance Distribution\")\n",
        "    plt.xlabel(\"Variance\")\n",
        "    plt.ylabel(\"Count of bits\")\n",
        "    plt.tight_layout()\n",
        "    out1 = os.path.join(FIG_DIR, f\"{fname_prefix}_variance_hist.png\")\n",
        "    plt.savefig(out1, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[saved] {out1}\")\n",
        "\n",
        "    # 2) Kept vs dropped bar\n",
        "    kept = int(kept_mask.sum())\n",
        "    dropped = int((~kept_mask).sum())\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.bar([\"kept\", \"dropped\"], [kept, dropped])\n",
        "    plt.title(f\"{title_prefix}: Kept vs Dropped (var>{var_threshold})\")\n",
        "    plt.tight_layout()\n",
        "    out2 = os.path.join(FIG_DIR, f\"{fname_prefix}_kept_dropped.png\")\n",
        "    plt.savefig(out2, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[saved] {out2}\")\n",
        "\n",
        "def plot_pca_scree(pca_model, fname=\"fp_pca_explained_variance.png\"):\n",
        "    if pca_model is None: return\n",
        "    evr = pca_model.explained_variance_ratio_; cum = np.cumsum(evr)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(range(1, len(evr)+1), evr, marker=\"o\")\n",
        "    plt.plot(range(1, len(evr)+1), cum, marker=\".\")\n",
        "    plt.xlabel(\"PCA component\"); plt.ylabel(\"Explained variance (ratio)\")\n",
        "    plt.title(\"FP PCA: explained variance (per-comp and cumulative)\")\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(FIG_DIR, fname); plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 7) Outlier detection and removal\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def log_iqr_bounds(s: pd.Series, fence: float = 1.5):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    s = s[s > 0]  # log requires positive\n",
        "    if s.empty:\n",
        "        return None, None\n",
        "    x = np.log10(s)\n",
        "    q1, q3 = np.nanpercentile(x, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lo, hi = q1 - fence * iqr, q3 + fence * iqr\n",
        "    return lo, hi  # bounds on log10 scale\n",
        "\n",
        "def flag_outliers_log_iqr(s: pd.Series, fence: float = 1.5) -> pd.Series:\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    mask_pos = s > 0\n",
        "    out = pd.Series(False, index=s.index, dtype=bool)\n",
        "    if not mask_pos.any():\n",
        "        return out\n",
        "    x = np.log10(s[mask_pos])\n",
        "    q1, q3 = np.nanpercentile(x, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lo, hi = q1 - fence * iqr, q3 + fence * iqr\n",
        "    bad = (x < lo) | (x > hi)\n",
        "    out.loc[bad.index] = bad.values\n",
        "    return out.astype(bool)\n",
        "\n",
        "def percentile_clip(s: pd.Series, lo=0.1, hi=99.9):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    p_lo, p_hi = np.nanpercentile(s.dropna(), [lo, hi]) if s.notna().any() else (np.nan, np.nan)\n",
        "    return s.clip(lower=p_lo, upper=p_hi)\n",
        "\n",
        "# ---------- 1) domain-rule mask ----------\n",
        "def domain_outlier_mask(df: pd.DataFrame):\n",
        "    m = pd.Series(False, index=df.index)\n",
        "    if \"pH_value\" in df:\n",
        "        m |= (df[\"pH_value\"] <= 0) | (df[\"pH_value\"] > 14)\n",
        "    if \"temperature_C\" in df:\n",
        "        m |= (df[\"temperature_C\"] < -10) | (df[\"temperature_C\"] > 120)\n",
        "    if \"kcat_value\" in df:\n",
        "        m |= (df[\"kcat_value\"] <= 0)\n",
        "    if \"km_value\" in df:\n",
        "        m |= (df[\"km_value\"] <= 0)\n",
        "    if \"unknown_frac\" in df:\n",
        "        m |= (df[\"unknown_frac\"] > 0.2)\n",
        "    return m\n",
        "\n",
        "# ---------- 2) log-IQR mask on kinetics ----------\n",
        "# def kinetics_outlier_mask(df: pd.DataFrame, fence=1.5, per_group=None) -> pd.Series:\n",
        "#     cols = [c for c in [\"kcat_value\", \"km_value\"] if c in df.columns]\n",
        "#     out = pd.Series(False, index=df.index, dtype=bool)\n",
        "\n",
        "#     def mask_for_subset(sub):\n",
        "#         m = pd.Series(False, index=sub.index, dtype=bool)\n",
        "#         for c in cols:\n",
        "#             m |= flag_outliers_log_iqr(sub[c], fence=fence)\n",
        "#         if {\"kcat_value\",\"km_value\"}.issubset(sub.columns):\n",
        "#             ratio = sub[\"kcat_value\"] / sub[\"km_value\"]\n",
        "#             m |= flag_outliers_log_iqr(ratio, fence=fence)\n",
        "#         return m.astype(bool)\n",
        "\n",
        "#     if per_group and per_group in df.columns:\n",
        "#         for _, sub in df.groupby(per_group):\n",
        "#             mg = mask_for_subset(sub)\n",
        "#             out.loc[mg.index] |= mg\n",
        "#     else:\n",
        "#         out |= mask_for_subset(df)\n",
        "\n",
        "#     # ensure strictly boolean, NaN->False\n",
        "#     return out.fillna(False).astype(bool)\n",
        "\n",
        "def kinetics_outlier_mask(\n",
        "    df: pd.DataFrame,\n",
        "    fence: float = 1.5,\n",
        "    # per_group: str | None = \"enzyme\",\n",
        "    per_group: Optional[str] = \"enzyme\",   \n",
        "    *,\n",
        "    adaptive: bool = True,\n",
        "    target_rate: float = 0.05,                 # aim to flag ≤ 5% per group\n",
        "    fence_grid: tuple = (1.5, 2.0, 2.5, 3.0, 3.5, 4.0),\n",
        "    min_group_size: int = 50,                  # don’t adapt tiny groups\n",
        "    use_ratio: bool = True,\n",
        "    verbose: bool = False\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Kinetics outlier mask using log-IQR on kcat, km (and optionally kcat/km).\n",
        "    - Operates per-group (e.g., per enzyme) when `per_group` is provided.\n",
        "    - If `adaptive=True`, it *loosens* the IQR fence within `fence_grid`\n",
        "      until the outlier rate ≤ target_rate for that group.\n",
        "    - Falls back to MAD rule if the grid can’t hit target_rate (rare on tiny/peculiar groups).\n",
        "    Returns a boolean Series indexed like df (True = outlier).\n",
        "    \"\"\"\n",
        "    def _log_values(s: pd.Series) -> pd.Series:\n",
        "        s = pd.to_numeric(s, errors=\"coerce\")\n",
        "        s = s[(s > 0) & np.isfinite(s)]\n",
        "        return np.log10(s)\n",
        "\n",
        "    def _iqr_mask(x_log: pd.Series, f: float) -> pd.Series:\n",
        "        if x_log.empty:\n",
        "            return pd.Series(False, index=x_log.index)\n",
        "        q1, q3 = np.nanpercentile(x_log, [25, 75])\n",
        "        iqr = q3 - q1\n",
        "        lo, hi = q1 - f * iqr, q3 + f * iqr\n",
        "        return (x_log < lo) | (x_log > hi)\n",
        "\n",
        "    def _mad_mask(x_log: pd.Series, k: float = 5.0) -> pd.Series:\n",
        "        # robust fallback: |x - median| / (1.4826 * MAD) > k\n",
        "        if x_log.empty:\n",
        "            return pd.Series(False, index=x_log.index)\n",
        "        med = np.nanmedian(x_log)\n",
        "        mad = np.nanmedian(np.abs(x_log - med))\n",
        "        if mad == 0 or not np.isfinite(mad):\n",
        "            return pd.Series(False, index=x_log.index)\n",
        "        z = np.abs(x_log - med) / (1.4826 * mad)\n",
        "        return z > k\n",
        "\n",
        "    # Prepare columns\n",
        "    have_kcat = \"kcat_value\" in df.columns\n",
        "    have_km   = \"km_value\" in df.columns\n",
        "    if not (have_kcat and have_km):\n",
        "        return pd.Series(False, index=df.index, dtype=bool)\n",
        "\n",
        "    # Precompute ratio safely\n",
        "    ratio = None\n",
        "    if use_ratio:\n",
        "        num = pd.to_numeric(df[\"kcat_value\"], errors=\"coerce\")\n",
        "        den = pd.to_numeric(df[\"km_value\"],   errors=\"coerce\")\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            ratio = num / den\n",
        "        ratio[~np.isfinite(ratio)] = np.nan\n",
        "\n",
        "    # Work either globally or per group\n",
        "    groups = [(None, df)] if not per_group or per_group not in df.columns else df.groupby(per_group, dropna=False)\n",
        "\n",
        "    out_mask = pd.Series(False, index=df.index, dtype=bool)\n",
        "\n",
        "    for gname, sub in groups:\n",
        "        idx = sub.index\n",
        "        x_kcat = _log_values(sub[\"kcat_value\"])\n",
        "        x_km   = _log_values(sub[\"km_value\"])\n",
        "        x_ratio = _log_values(ratio.loc[idx]) if (use_ratio and ratio is not None) else pd.Series([], dtype=float)\n",
        "\n",
        "        # If too small, just use global fence (no adaptation)\n",
        "        adapt = adaptive and (len(idx) >= min_group_size)\n",
        "\n",
        "        # Try fences from tight → loose until ≤ target_rate\n",
        "        chosen_fence = fence\n",
        "        def _compose_mask(f):\n",
        "            m = pd.Series(False, index=idx, dtype=bool)\n",
        "            if not x_kcat.empty: m |= _iqr_mask(x_kcat, f).reindex(idx, fill_value=False)\n",
        "            if not x_km.empty:   m |= _iqr_mask(x_km,   f).reindex(idx, fill_value=False)\n",
        "            if not x_ratio.empty: m |= _iqr_mask(x_ratio, f).reindex(idx, fill_value=False)\n",
        "            return m\n",
        "\n",
        "        if adapt:\n",
        "            m_try = None\n",
        "            for f in fence_grid:\n",
        "                m_tmp = _compose_mask(f)\n",
        "                rate = m_tmp.mean() if len(m_tmp) else 0.0\n",
        "                if rate <= target_rate:\n",
        "                    chosen_fence, m_try = f, m_tmp\n",
        "                    break\n",
        "            if m_try is None:\n",
        "                # fallback to MAD-based rule when grid can’t achieve target\n",
        "                m_mad = pd.Series(False, index=idx, dtype=bool)\n",
        "                if not x_kcat.empty:  m_mad |= _mad_mask(x_kcat).reindex(idx, fill_value=False)\n",
        "                if not x_km.empty:    m_mad |= _mad_mask(x_km).reindex(idx, fill_value=False)\n",
        "                if not x_ratio.empty: m_mad |= _mad_mask(x_ratio).reindex(idx, fill_value=False)\n",
        "                m_try = m_mad\n",
        "            m_g = m_try\n",
        "        else:\n",
        "            m_g = _compose_mask(fence)\n",
        "\n",
        "        out_mask.loc[idx] = m_g\n",
        "\n",
        "        if verbose:\n",
        "            n = len(idx)\n",
        "            r = m_g.mean()*100 if n else 0.0\n",
        "            print(f\"[kinetics] group={gname!r:>12} n={n:6d}  fence={chosen_fence:.1f}{' (MAD)' if adapt and m_try is m_g and chosen_fence==fence and r>target_rate*100 else ''}  outliers={int(m_g.sum()):5d} ({r:4.1f}%)\")\n",
        "\n",
        "    # Ensure boolean with NaN→False\n",
        "    return out_mask.fillna(False).astype(bool)\n",
        "\n",
        "def compute_sequential_drop_masks(\n",
        "    df: pd.DataFrame,\n",
        "    fence: float = 1.5,\n",
        "    contamination: float = 0.01,\n",
        "    per_group: str = \"enzyme\",\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Return boolean Series (aligned to df.index) that mark rows *actually dropped*\n",
        "    by apply_outlier_strategy order: domain → kinetics → iso.\n",
        "    Keys: 'domain_drop', 'kinetics_drop', 'iso_drop', 'kept'\n",
        "    \"\"\"\n",
        "    # A) domain\n",
        "    m_domain = domain_outlier_mask(df).fillna(False).astype(bool)\n",
        "\n",
        "    # B) kinetics on the remaining rows\n",
        "    df1 = df[~m_domain].copy()\n",
        "    m_kin_sub = kinetics_outlier_mask(df1, fence=fence, per_group=per_group)\n",
        "    m_kin = pd.Series(False, index=df.index, dtype=bool)\n",
        "    m_kin.loc[df1.index] = m_kin_sub.values\n",
        "\n",
        "    # C) iso on the remaining rows\n",
        "    df2 = df1[~m_kin_sub].copy()\n",
        "    cont_cols = df2.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cont_cols = [c for c in cont_cols if not c.startswith(\"fp_\")]\n",
        "    m_iso_sub = iso_forest_mask(df2, cont_cols, cont_rate=contamination)\n",
        "    m_iso = pd.Series(False, index=df.index, dtype=bool)\n",
        "    m_iso.loc[df2.index] = m_iso_sub.values\n",
        "\n",
        "    kept = ~(m_domain | m_kin | m_iso)\n",
        "    return {\n",
        "        \"domain_drop\": m_domain,\n",
        "        \"kinetics_drop\": m_kin,\n",
        "        \"iso_drop\": m_iso,\n",
        "        \"kept\": kept,\n",
        "    }\n",
        "\n",
        "def plot_kcat_km_scatter_drops(\n",
        "    df: pd.DataFrame,\n",
        "    drop_masks: dict,                     # from compute_sequential_drop_masks(...)\n",
        "    fname: str = \"scatter_kcat_km_drops.png\",\n",
        "    show_regular: bool = True,            # draw kept points as a 4th color\n",
        "    sample_regular: Optional[int] = 5000, # None=all kept, int=subsample, 0=hide\n",
        "    regular_alpha: float = 0.5\n",
        "):\n",
        "    # --- filter to values we can actually plot on log axes ---\n",
        "    d = df[(pd.to_numeric(df[\"kcat_value\"], errors=\"coerce\") > 0) &\n",
        "           (pd.to_numeric(df[\"km_value\"],   errors=\"coerce\") > 0)].copy()\n",
        "    d[\"log_kcat\"] = np.log10(d[\"kcat_value\"])\n",
        "    d[\"log_km\"]   = np.log10(d[\"km_value\"])\n",
        "\n",
        "    # reindex masks to plotted subset\n",
        "    m_dom_all = drop_masks[\"domain_drop\"]   # full-data mask\n",
        "    m_kin_all = drop_masks[\"kinetics_drop\"]\n",
        "    m_iso_all = drop_masks[\"iso_drop\"]\n",
        "    m_keep_all= drop_masks[\"kept\"]\n",
        "\n",
        "    # totals across full df (for legend)\n",
        "    n_dom_all = int(m_dom_all.sum())\n",
        "    n_kin_all = int(m_kin_all.sum())\n",
        "    n_iso_all = int(m_iso_all.sum())\n",
        "    n_keep_all= int(m_keep_all.sum())\n",
        "\n",
        "    # masks restricted to the scatterable subset\n",
        "    m_dom = m_dom_all.reindex(d.index, fill_value=False)\n",
        "    m_kin = m_kin_all.reindex(d.index, fill_value=False)\n",
        "    m_iso = m_iso_all.reindex(d.index, fill_value=False)\n",
        "    m_keep= m_keep_all.reindex(d.index, fill_value=False)\n",
        "\n",
        "    # counts that are actually being plotted\n",
        "    n_dom_plot = int(m_dom.sum())\n",
        "    n_kin_plot = int(m_kin.sum())\n",
        "    n_iso_plot = int(m_iso.sum())\n",
        "    n_keep_plot= int(m_keep.sum())\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,6))\n",
        "\n",
        "    # dropped by domain\n",
        "    idx_dom = d.index[m_dom]\n",
        "    if len(idx_dom):\n",
        "        plt.scatter(d.loc[idx_dom, \"log_kcat\"], d.loc[idx_dom, \"log_km\"],\n",
        "                    s=3, alpha=0.9, color=\"#A93226\",\n",
        "                    label=f\"domain ({n_dom_plot:,} / {n_dom_all:,})\")\n",
        "\n",
        "    # dropped by kinetics\n",
        "    idx_kin = d.index[m_kin]\n",
        "    if len(idx_kin):\n",
        "        plt.scatter(d.loc[idx_kin, \"log_kcat\"], d.loc[idx_kin, \"log_km\"],\n",
        "                    s=2, alpha=0.75, color=\"#E67E22\",\n",
        "                    label=f\"kinetics ({n_kin_plot:,} / {n_kin_all:,})\")\n",
        "\n",
        "    # dropped by iso\n",
        "    idx_iso = d.index[m_iso]\n",
        "    if len(idx_iso):\n",
        "        plt.scatter(d.loc[idx_iso, \"log_kcat\"], d.loc[idx_iso, \"log_km\"],\n",
        "                    s=2, alpha=0.8, color=\"#93910F\",\n",
        "                    label=f\"iso ({n_iso_plot:,} / {n_iso_all:,})\")\n",
        "        \n",
        "    # regular (kept) points, lightly\n",
        "    if show_regular and n_keep_plot:\n",
        "        if isinstance(sample_regular, int) and sample_regular > 0 and n_keep_plot > sample_regular:\n",
        "            idx = d.index[m_keep].to_series().sample(sample_regular, random_state=42).index\n",
        "        elif sample_regular == 0:\n",
        "            idx = pd.Index([])\n",
        "        else:\n",
        "            idx = d.index[m_keep]\n",
        "        if len(idx):\n",
        "            plt.scatter(d.loc[idx, \"log_kcat\"], d.loc[idx, \"log_km\"],\n",
        "                        s=2, alpha=regular_alpha, color=\"#1F77B4\",\n",
        "                        label=f\"kept (shown {len(idx):,} / total {n_keep_all:,})\")\n",
        "\n",
        "    # y=x guide\n",
        "    lo = np.nanmin(d[[\"log_kcat\",\"log_km\"]].values)\n",
        "    hi = np.nanmax(d[[\"log_kcat\",\"log_km\"]].values)\n",
        "    if np.isfinite(lo) and np.isfinite(hi):\n",
        "        plt.plot([lo,hi], [lo,hi], linestyle=\"--\", linewidth=1, color=\"#7f8c8d\", alpha=0.6)\n",
        "\n",
        "    plt.xlabel(\"log10(kcat [s⁻¹])\")\n",
        "    plt.ylabel(\"log10(Km)\")\n",
        "    plt.title(\"kcat vs Km — dropped points by reason (sequential strategy)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(frameon=True)\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=220, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(\"[saved]\", out)\n",
        "\n",
        "\n",
        "# def plot_kcat_km_scatter_drops(\n",
        "#     df: pd.DataFrame,\n",
        "#     drop_masks: dict,\n",
        "#     fname: str = \"scatter_kcat_km_drops.png\",\n",
        "#     show_regular: bool = False,   # set False to hide regular points\n",
        "#     # sample_regular: int | None = 5000,  # None=plot all kept; 0=hide; int=subsample\n",
        "#     sample_regular: Optional[int] = 5000,   \n",
        "# ):\n",
        "#     # need kcat/km > 0 for log axes\n",
        "#     d = df[(df[\"kcat_value\"] > 0) & (df[\"km_value\"] > 0)].copy()\n",
        "#     d[\"log_kcat\"] = np.log10(d[\"kcat_value\"])\n",
        "#     d[\"log_km\"]   = np.log10(d[\"km_value\"])\n",
        "\n",
        "#     # align masks to the filtered d\n",
        "#     m_dom = drop_masks[\"domain_drop\"].reindex(d.index, fill_value=False)\n",
        "#     m_kin = drop_masks[\"kinetics_drop\"].reindex(d.index, fill_value=False)\n",
        "#     m_iso = drop_masks[\"iso_drop\"].reindex(d.index, fill_value=False)\n",
        "#     m_keep = drop_masks[\"kept\"].reindex(d.index, fill_value=False)\n",
        "\n",
        "#     plt.figure(figsize=(7,6))\n",
        "\n",
        "#     # (optional) regular points\n",
        "#     if show_regular:\n",
        "#         if isinstance(sample_regular, int) and sample_regular > 0 and m_keep.sum() > sample_regular:\n",
        "#             idx = d.index[m_keep].to_series().sample(sample_regular, random_state=42).index\n",
        "#         elif sample_regular == 0:\n",
        "#             idx = pd.Index([])\n",
        "#         else:\n",
        "#             idx = d.index[m_keep]\n",
        "#         if len(idx):\n",
        "#             plt.scatter(d.loc[idx, \"log_kcat\"], d.loc[idx, \"log_km\"],\n",
        "#                         s=6, alpha=0.15, color=\"#BBBBBB\", label=f\"regular (n={len(idx):,})\")\n",
        "\n",
        "#     # dropped by domain\n",
        "#     idx_dom = d.index[m_dom]\n",
        "#     if len(idx_dom):\n",
        "#         plt.scatter(d.loc[idx_dom, \"log_kcat\"], d.loc[idx_dom, \"log_km\"],\n",
        "#                     s=14, alpha=0.9, color=\"#B22222\", label=f\"domain (n={len(idx_dom):,})\")\n",
        "\n",
        "#     # dropped by kinetics\n",
        "#     idx_kin = d.index[m_kin]\n",
        "#     if len(idx_kin):\n",
        "#         plt.scatter(d.loc[idx_kin, \"log_kcat\"], d.loc[idx_kin, \"log_km\"],\n",
        "#                     s=10, alpha=0.75, color=\"#E67E22\", label=f\"kinetics (n={len(idx_kin):,})\")\n",
        "\n",
        "#     # dropped by iso\n",
        "#     idx_iso = d.index[m_iso]\n",
        "#     if len(idx_iso):\n",
        "#         plt.scatter(d.loc[idx_iso, \"log_kcat\"], d.loc[idx_iso, \"log_km\"],\n",
        "#                     s=10, alpha=0.8, color=\"#1F77B4\", label=f\"iso (n={len(idx_iso):,})\")\n",
        "\n",
        "#     # y=x guide\n",
        "#     lo = np.nanmin(d[[\"log_kcat\",\"log_km\"]].values)\n",
        "#     hi = np.nanmax(d[[\"log_kcat\",\"log_km\"]].values)\n",
        "#     if np.isfinite(lo) and np.isfinite(hi):\n",
        "#         plt.plot([lo,hi], [lo,hi], linestyle=\"--\", linewidth=1, color=\"#7f8c8d\", alpha=0.6)\n",
        "\n",
        "#     plt.xlabel(\"log10(kcat [s⁻¹])\")\n",
        "#     plt.ylabel(\"log10(Km)\")\n",
        "#     plt.title(\"kcat vs Km — dropped points by reason (sequential strategy)\")\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     plt.legend(frameon=True)\n",
        "#     out = os.path.join(FIG_DIR, fname)\n",
        "#     plt.savefig(out, dpi=220, bbox_inches=\"tight\"); plt.close()\n",
        "#     print(\"[saved]\", out)\n",
        "\n",
        "# ---------- 3) IsolationForest on continuous block ----------\n",
        "def iso_forest_mask(df: pd.DataFrame, feature_cols: list, cont_rate=0.01, random_state=42):\n",
        "    X = df[feature_cols].select_dtypes(include=[np.number]).copy()\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
        "    if X.empty: \n",
        "        return pd.Series(False, index=df.index)\n",
        "    iso = IsolationForest(contamination=cont_rate, random_state=random_state)\n",
        "    scores = iso.fit_predict(X)  # -1 = outlier\n",
        "    return pd.Series(scores == -1, index=df.index)\n",
        "\n",
        "# ---------- Apply in your pipeline ----------\n",
        "def apply_outlier_strategy(df: pd.DataFrame):\n",
        "    df = df.copy()\n",
        "\n",
        "    mask_domain = domain_outlier_mask(df).fillna(False).astype(bool)\n",
        "    print(f\"[Outliers] domain-rule drops: {mask_domain.sum()}\")\n",
        "    df = df[~mask_domain].copy()\n",
        "\n",
        "    mask_kin = kinetics_outlier_mask(df, fence=1.5, per_group=(\"enzyme\" if \"enzyme\" in df.columns else None))\n",
        "    mask_kin = mask_kin.fillna(False).astype(bool)\n",
        "    print(f\"[Outliers] kinetics log-IQR drops: {mask_kin.sum()}\")\n",
        "    df = df[~mask_kin].copy()\n",
        "\n",
        "    # (optional) winsorize descriptors here...\n",
        "\n",
        "    cont_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cont_cols = [c for c in cont_cols if not c.startswith(\"fp_\")]\n",
        "    mask_iso = iso_forest_mask(df, cont_cols, cont_rate=0.01).fillna(False).astype(bool)\n",
        "    print(f\"[Outliers] isolation-forest drops: {mask_iso.sum()}\")\n",
        "    df = df[~mask_iso].copy()\n",
        "\n",
        "    return df\n",
        "\n",
        "# ========= 1) Outlier masks with metadata for plotting =========\n",
        "\n",
        "def compute_log_iqr_bounds(s: pd.Series, fence: float = 1.5):\n",
        "    \"\"\"Bounds on log10 scale; returns (lo, hi, valid_index) or (None, None, empty_index).\"\"\"\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    s = s[s > 0]\n",
        "    if s.empty:\n",
        "        return None, None, s.index\n",
        "    x = np.log10(s)\n",
        "    q1, q3 = np.nanpercentile(x, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lo, hi = q1 - fence * iqr, q3 + fence * iqr\n",
        "    return lo, hi, s.index\n",
        "\n",
        "def log_iqr_mask_and_bounds(series: pd.Series, fence: float = 1.5):\n",
        "    \"\"\"Return (mask, bounds_dict) where mask is boolean (NaN→False).\"\"\"\n",
        "    lo, hi, idx = compute_log_iqr_bounds(series, fence=fence)\n",
        "    mask = pd.Series(False, index=series.index, dtype=bool)\n",
        "    bnds = {\"lo\": lo, \"hi\": hi, \"valid_idx\": idx}\n",
        "    if lo is None:\n",
        "        return mask, bnds\n",
        "    x = np.log10(series[series > 0])\n",
        "    bad = (x < lo) | (x > hi)\n",
        "    mask.loc[bad.index] = True\n",
        "    return mask.astype(bool), bnds\n",
        "\n",
        "def iso_forest_scores(df: pd.DataFrame, feature_cols: list, contamination=0.01, random_state=42):\n",
        "    \"\"\"Return (is_outlier_mask, scores, threshold). Score is anomaly ‘decision_function’ (higher is more normal).\"\"\"\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    X = df[feature_cols].select_dtypes(include=[np.number]).copy()\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
        "    if X.empty:\n",
        "        return pd.Series(False, index=df.index, dtype=bool), pd.Series(np.nan, index=df.index), np.nan\n",
        "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    iso.fit(X)\n",
        "    # decision_function: larger → more normal; smaller → more anomalous\n",
        "    score = pd.Series(iso.decision_function(X), index=df.index)\n",
        "    # threshold chosen by contamination quantile\n",
        "    thr = score.quantile(contamination)\n",
        "    mask = score < thr\n",
        "    return mask.astype(bool), score, float(thr)\n",
        "\n",
        "def compute_outlier_masks_with_meta(df: pd.DataFrame, fence=1.5, contamination=0.01):\n",
        "    \"\"\"Compute masks for each rule + store bounds/scores for visualization.\"\"\"\n",
        "    # A) domain\n",
        "    m_domain = domain_outlier_mask(df).fillna(False).astype(bool)\n",
        "\n",
        "    # B) kinetics log-IQR (global); also store bounds for plots\n",
        "    m_kcat, b_kcat = log_iqr_mask_and_bounds(df.get(\"kcat_value\", pd.Series(index=df.index)), fence)\n",
        "    m_km,   b_km   = log_iqr_mask_and_bounds(df.get(\"km_value\", pd.Series(index=df.index)), fence)\n",
        "    ratio = (df[\"kcat_value\"] / df[\"km_value\"]) if {\"kcat_value\",\"km_value\"}.issubset(df.columns) else pd.Series(index=df.index)\n",
        "    m_ratio, b_ratio = log_iqr_mask_and_bounds(ratio, fence)\n",
        "    m_kin = (m_kcat | m_km | m_ratio).astype(bool)\n",
        "\n",
        "    # C) IsolationForest on continuous descriptors (exclude fingerprints)\n",
        "    cont_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cont_cols = [c for c in cont_cols if not c.startswith(\"fp_\")]\n",
        "    m_iso, iso_score, iso_thr = iso_forest_scores(df, cont_cols, contamination=contamination)\n",
        "\n",
        "    meta = {\n",
        "        \"bounds\": {\"kcat\": b_kcat, \"km\": b_km, \"kcat_over_km\": b_ratio},\n",
        "        \"iso\": {\"score\": iso_score, \"thr\": iso_thr, \"features_used\": cont_cols, \"contamination\": contamination},\n",
        "    }\n",
        "    masks = {\"domain\": m_domain, \"kinetics\": m_kin, \"iso\": m_iso}\n",
        "    return masks, meta\n",
        "\n",
        "\n",
        "\n",
        "# ========= 2) Visualization helpers =========\n",
        "\n",
        "def plot_umap_fingerprints(enzy_data: pd.DataFrame, label_col: str = None, n_neighbors: int = 15, min_dist: float = 0.1, fname: str = \"umap_fingerprints.png\"):\n",
        "    \"\"\"\n",
        "    Visualize RDKit fingerprints (fp_* columns) in 2D using UMAP with Jaccard distance.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    enzy_data : pd.DataFrame\n",
        "        DataFrame containing fingerprint columns prefixed with 'fp_'.\n",
        "    label_col : str, optional\n",
        "        Column name in enzy_data to color points by (e.g., enzyme type, substrate, cluster).\n",
        "        If None, all points are plotted in one color.\n",
        "    n_neighbors : int\n",
        "        UMAP parameter controlling local vs global structure balance.\n",
        "    min_dist : float\n",
        "        UMAP parameter controlling how tightly points cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1️⃣ Extract fingerprint columns\n",
        "    fp_cols = [c for c in enzy_data.columns if c.startswith(\"fp_\")]\n",
        "    X_fp = enzy_data[fp_cols].astype(np.uint8).values  # ensure binary int type\n",
        "\n",
        "    # 2️⃣ Run UMAP (Jaccard metric for binary data)\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        metric=\"jaccard\",\n",
        "        random_state=42\n",
        "    )\n",
        "    embedding = reducer.fit_transform(X_fp)\n",
        "\n",
        "    # 3️⃣ Build plot DataFrame\n",
        "    plot_df = pd.DataFrame(embedding, columns=[\"UMAP_1\", \"UMAP_2\"])\n",
        "    if label_col and label_col in enzy_data.columns:\n",
        "        plot_df[label_col] = enzy_data[label_col]\n",
        "\n",
        "    # 4️⃣ Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    if label_col and label_col in plot_df:\n",
        "        scatter = plt.scatter(\n",
        "            plot_df[\"UMAP_1\"],\n",
        "            plot_df[\"UMAP_2\"],\n",
        "            c=pd.Categorical(plot_df[label_col]).codes,\n",
        "            cmap=\"Spectral\",\n",
        "            s=4,\n",
        "            alpha=0.7\n",
        "        )\n",
        "        plt.colorbar(scatter, label=label_col)\n",
        "    else:\n",
        "        plt.scatter(plot_df[\"UMAP_1\"], plot_df[\"UMAP_2\"], s=3, alpha=0.6, color=\"steelblue\")\n",
        "    \n",
        "    plt.title(\"UMAP projection of Morgan fingerprints (Jaccard metric)\")\n",
        "    plt.xlabel(\"UMAP 1\")\n",
        "    plt.ylabel(\"UMAP 2\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def _log_hist_with_fences(ax, series, title, bounds, removed_mask):\n",
        "    \"\"\"\n",
        "    Plot log10 histogram with IQR fences + vertical lines at min and max values.\n",
        "    Safely ignores non-finite or ≤0 entries.\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    s = s[(s > 0) & np.isfinite(s)]\n",
        "    if s.empty:\n",
        "        ax.set_title(f\"{title} (no valid values)\")\n",
        "        ax.set_xlabel(\"log10 value\")\n",
        "        ax.set_ylabel(\"count\")\n",
        "        return\n",
        "\n",
        "    x = np.log10(s)\n",
        "    rem = removed_mask.reindex(x.index, fill_value=False)\n",
        "\n",
        "    nbins = min(80, max(20, int(np.sqrt(len(x)))))\n",
        "    ax.hist(x, bins=nbins, alpha=0.7, color='steelblue', edgecolor='gray')\n",
        "    \n",
        "    # Draw IQR fences if valid\n",
        "    lo = bounds.get(\"lo\", None)\n",
        "    hi = bounds.get(\"hi\", None)\n",
        "    if lo is not None and np.isfinite(lo):\n",
        "        ax.axvline(lo, color=\"orange\", linestyle=\"--\", label=\"IQR low fence\")\n",
        "    if hi is not None and np.isfinite(hi):\n",
        "        ax.axvline(hi, color=\"orange\", linestyle=\"--\", label=\"IQR high fence\")\n",
        "\n",
        "    # Draw min/max vertical lines\n",
        "    xmin, xmax = x.min(), x.max()\n",
        "    ax.axvline(xmin, color=\"green\", linestyle=\":\", label=\"min\")\n",
        "    ax.axvline(xmax, color=\"red\", linestyle=\":\", label=\"max\")\n",
        "\n",
        "    ax.set_title(f\"{title} (removed={int(rem.sum())})\")\n",
        "    ax.set_xlabel(\"log10 value\")\n",
        "    ax.set_ylabel(\"count\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "def plot_outlier_histograms(df, masks, meta, fname_prefix=\"outlier_hists\"):\n",
        "    plt.figure(figsize=(14,4))\n",
        "\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    _log_hist_with_fences(ax1, df.get(\"kcat_value\"), \"kcat_value\",\n",
        "                          meta[\"bounds\"][\"kcat\"], masks[\"kinetics\"])\n",
        "\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    _log_hist_with_fences(ax2, df.get(\"km_value\"), \"km_value\",\n",
        "                          meta[\"bounds\"][\"km\"], masks[\"kinetics\"])\n",
        "\n",
        "    # ratio: guard against division by zero / infs\n",
        "    if {\"kcat_value\", \"km_value\"}.issubset(df.columns):\n",
        "        ratio = pd.to_numeric(df[\"kcat_value\"], errors=\"coerce\") / \\\n",
        "                pd.to_numeric(df[\"km_value\"], errors=\"coerce\")\n",
        "        ratio[~np.isfinite(ratio)] = np.nan\n",
        "    else:\n",
        "        ratio = pd.Series(index=df.index, dtype=float)\n",
        "\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    _log_hist_with_fences(ax3, ratio, \"kcat/km\",\n",
        "                          meta[\"bounds\"][\"kcat_over_km\"], masks[\"kinetics\"])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(FIG_DIR, f\"{fname_prefix}.png\")\n",
        "    plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "def plot_kcat_km_scatter_with_outliers(df, masks, fname=\"scatter_kcat_km_outliers.png\"):\n",
        "    if not {\"kcat_value\",\"km_value\"}.issubset(df.columns):\n",
        "        return\n",
        "    d = df[(df[\"kcat_value\"]>0) & (df[\"km_value\"]>0)].copy()\n",
        "    d[\"log_kcat\"] = np.log10(d[\"kcat_value\"])\n",
        "    d[\"log_km\"]   = np.log10(d[\"km_value\"])\n",
        "    removed = masks[\"kinetics\"] | masks[\"domain\"]\n",
        "    plt.figure(figsize=(6,5))\n",
        "    # kept points\n",
        "    keep = ~removed.reindex(d.index, fill_value=False)\n",
        "    plt.scatter(d.loc[keep,\"log_kcat\"], d.loc[keep,\"log_km\"], s=6, alpha=0.35)\n",
        "    # removed points\n",
        "    rem = removed.reindex(d.index, fill_value=False)\n",
        "    if rem.any():\n",
        "        plt.scatter(d.loc[rem,\"log_kcat\"], d.loc[rem,\"log_km\"], s=10, alpha=0.8)\n",
        "    plt.xlabel(\"log10(kcat)\"); plt.ylabel(\"log10(Km)\")\n",
        "    plt.title(\"kcat vs Km (removed points highlighted)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "def plot_kcat_km_scatter_by_reason(\n",
        "    df,\n",
        "    masks,\n",
        "    sample=80000,\n",
        "    fname=\"scatter_kcat_km_by_reason.png\",\n",
        "    priority=(\"domain\",\"kinetics\",\"iso\"),\n",
        "):\n",
        "    \"\"\"\n",
        "    Single scatter of log10(kcat) vs log10(Km) with four classes:\n",
        "      - domain (color A)\n",
        "      - kinetics (color B)\n",
        "      - iso (color C)\n",
        "      - regular (none of the above, color D)\n",
        "\n",
        "    If a point matches multiple masks, priority determines its class (first match wins).\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- guard & prep ----\n",
        "    needed = {\"kcat_value\",\"km_value\"}\n",
        "    if not needed.issubset(df.columns):\n",
        "        return\n",
        "\n",
        "    d = df.copy()\n",
        "    d[\"kcat_value\"] = pd.to_numeric(d[\"kcat_value\"], errors=\"coerce\")\n",
        "    d[\"km_value\"]   = pd.to_numeric(d[\"km_value\"], errors=\"coerce\")\n",
        "    d = d[(d[\"kcat_value\"] > 0) & (d[\"km_value\"] > 0)]\n",
        "    if d.empty:\n",
        "        return\n",
        "\n",
        "    d[\"log_kcat\"] = np.log10(d[\"kcat_value\"])\n",
        "    d[\"log_km\"]   = np.log10(d[\"km_value\"])\n",
        "\n",
        "    # ---- build class labels with priority ----\n",
        "    # start as regular (False everywhere)\n",
        "    lab = pd.Series(\"regular\", index=d.index, dtype=object)\n",
        "\n",
        "    # apply priority: first match wins\n",
        "    for key in priority:\n",
        "        if key in masks:\n",
        "            m = masks[key].reindex(d.index, fill_value=False)\n",
        "            lab.loc[m & (lab == \"regular\")] = key\n",
        "\n",
        "    # ---- optional downsample for plotting (keep class balance) ----\n",
        "    if sample is not None and len(d) > sample:\n",
        "        idx_keep = []\n",
        "        for cls in [\"regular\"] + list(priority):\n",
        "            idx_cls = d[lab == cls].index\n",
        "            if len(idx_cls) == 0:\n",
        "                continue\n",
        "            # proportional sampling with a floor\n",
        "            frac = min(1.0, sample / len(d))\n",
        "            n_take = max(2000 if cls != \"regular\" else 4000, int(len(idx_cls) * frac))\n",
        "            n_take = min(n_take, len(idx_cls))\n",
        "            idx_keep.append(idx_cls.to_series().sample(n=n_take, random_state=42).index)\n",
        "        # idx_keep = idx_keep[0].union_many(idx_keep[1:]) if len(idx_keep) > 1 else idx_keep[0]\n",
        "        idx_keep = reduce(lambda a, b: a.union(b), idx_keep) if idx_keep else pd.Index([])\n",
        "        d_plot = d.loc[idx_keep]\n",
        "        lab_plot = lab.loc[idx_keep]\n",
        "    else:\n",
        "        d_plot = d\n",
        "        lab_plot = lab\n",
        "\n",
        "    # ---- counts (full, not sampled) ----\n",
        "    counts = {\n",
        "        \"domain\": int((lab == \"domain\").sum()),\n",
        "        \"kinetics\": int((lab == \"kinetics\").sum()),\n",
        "        \"iso\": int((lab == \"iso\").sum()),\n",
        "        \"regular\": int((lab == \"regular\").sum()),\n",
        "    }\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    # ---- colors & order (plot regular first so outliers overlay) ----\n",
        "    color_map = {\n",
        "        \"regular\": \"#c7c7c7\",  # light grey\n",
        "        \"domain\":  \"#d62728\",  # red\n",
        "        \"kinetics\":\"#ff7f0e\",  # orange\n",
        "        \"iso\":     \"#1f77b4\",  # blue\n",
        "    }\n",
        "    order = [\"regular\", \"domain\", \"kinetics\", \"iso\"]\n",
        "\n",
        "    plt.figure(figsize=(6.8, 5.6))\n",
        "\n",
        "    for cls in order:\n",
        "        mask_cls = (lab_plot == cls)\n",
        "        if mask_cls.any():\n",
        "            plt.scatter(\n",
        "                d_plot.loc[mask_cls, \"log_kcat\"],\n",
        "                d_plot.loc[mask_cls, \"log_km\"],\n",
        "                s=7 if cls == \"regular\" else 12,\n",
        "                alpha=0.25 if cls == \"regular\" else 0.85,\n",
        "                linewidths=0,\n",
        "                label=f\"{cls} (n={counts[cls]}, {counts[cls]/max(total,1)*100:.1f}%)\",\n",
        "                c=color_map[cls],\n",
        "            )\n",
        "\n",
        "    # y=x guide on log–log\n",
        "    xlim = plt.xlim(); ylim = plt.ylim()\n",
        "    lims = [min(xlim[0], ylim[0]), max(xlim[1], ylim[1])]\n",
        "    plt.plot(lims, lims, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
        "    plt.xlim(lims); plt.ylim(lims)\n",
        "\n",
        "    plt.xlabel(\"log10(kcat [s⁻¹])\")\n",
        "    plt.ylabel(\"log10(Km)\")\n",
        "    pr_txt = \" > \".join(priority) + \" > regular\"\n",
        "    plt.title(f\"kcat vs Km — colored by outlier reason (priority: {pr_txt})\")\n",
        "    plt.legend(loc=\"best\", framealpha=0.9, fontsize=9)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
        "    print(\"[saved]\", out)\n",
        "\n",
        "\n",
        "def plot_iso_score_hist(meta, masks, fname=\"iso_scores.png\"):\n",
        "    sc = meta[\"iso\"][\"score\"]\n",
        "    if sc.isna().all():\n",
        "        return\n",
        "    thr = meta[\"iso\"][\"thr\"]\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(sc.dropna(), bins=60)\n",
        "    plt.axvline(thr, linestyle=\"--\")\n",
        "    plt.title(f\"IsolationForest decision scores (cut@{meta['iso']['contamination']*100:.1f}%)\")\n",
        "    plt.xlabel(\"decision_function (higher=more normal)\"); plt.ylabel(\"count\")\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "def plot_outliers_pie(masks, fname=\"outliers_pie.png\"):\n",
        "    # Count rows by *first* reason (domain > kinetics > iso)\n",
        "    # so that a row dropped by domain isn’t double-counted.\n",
        "    idx = None\n",
        "    for k in [\"domain\",\"kinetics\",\"iso\"]:\n",
        "        m = masks[k]\n",
        "        idx = m[m].index if idx is None else idx.union(m[m].index)\n",
        "    counts = {\n",
        "        \"domain\": masks[\"domain\"].sum(),\n",
        "        \"kinetics\": (~masks[\"domain\"] & masks[\"kinetics\"]).sum(),\n",
        "        \"iso\": (~masks[\"domain\"] & ~masks[\"kinetics\"] & masks[\"iso\"]).sum(),\n",
        "    }\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.pie(list(counts.values()), labels=list(counts.keys()), autopct=\"%1.1f%%\", startangle=140, wedgeprops={\"edgecolor\":\"white\"})\n",
        "    plt.title(\"Outliers by reason\")\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200); plt.close(); print(\"[saved]\", out)\n",
        "\n",
        "\n",
        "# # ======================================================\n",
        "# # 7) Orchestration\n",
        "# # ======================================================\n",
        "\n",
        "def _to_log10_clean(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Coerce to numeric, keep strictly positive & finite, return log10 values.\"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    s = s[(s > 0) & np.isfinite(s)]\n",
        "    if s.empty:\n",
        "        return s  # empty series\n",
        "    return np.log10(s)\n",
        "\n",
        "def _log_hist_with_fences(ax, series, title, bounds, removed_mask):\n",
        "    \"\"\"\n",
        "    Plot log10 histogram with:\n",
        "      - IQR fences (low/high)\n",
        "      - min/max vertical lines\n",
        "      - legend showing their relative % positions\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    s = s[(s > 0) & np.isfinite(s)]\n",
        "    if s.empty:\n",
        "        ax.set_title(f\"{title} (no valid values)\")\n",
        "        ax.set_xlabel(\"log10 value\")\n",
        "        ax.set_ylabel(\"count\")\n",
        "        return\n",
        "\n",
        "    x = np.log10(s)\n",
        "    rem = removed_mask.reindex(x.index, fill_value=False)\n",
        "    nbins = min(80, max(20, int(np.sqrt(len(x)))))\n",
        "    ax.hist(x, bins=nbins, alpha=0.7, color='steelblue', edgecolor='gray')\n",
        "\n",
        "    xmin, xmax = x.min(), x.max()\n",
        "    xmid = (xmin + xmax) / 2\n",
        "    xspan = xmax - xmin if xmax > xmin else 1e-6\n",
        "\n",
        "    lo, hi = bounds.get(\"lo\", None), bounds.get(\"hi\", None)\n",
        "    lines, labels = [], []\n",
        "\n",
        "    # --- IQR fences ---\n",
        "    if lo is not None and np.isfinite(lo):\n",
        "        ax.axvline(lo, color=\"orange\", linestyle=\"--\", linewidth=1.5)\n",
        "        pos = (lo - xmin) / xspan * 100\n",
        "        lines.append(ax.axvline(lo, color=\"orange\", linestyle=\"--\", linewidth=1.5))\n",
        "        labels.append(f\"IQR low fence ({pos:.1f}%)\")\n",
        "\n",
        "    if hi is not None and np.isfinite(hi):\n",
        "        ax.axvline(hi, color=\"orange\", linestyle=\"--\", linewidth=1.5)\n",
        "        pos = (hi - xmin) / xspan * 100\n",
        "        lines.append(ax.axvline(hi, color=\"orange\", linestyle=\"--\", linewidth=1.5))\n",
        "        labels.append(f\"IQR high fence ({pos:.1f}%)\")\n",
        "\n",
        "    # --- Min/Max lines ---\n",
        "    lmin = ax.axvline(xmin, color=\"green\", linestyle=\":\", linewidth=1.5)\n",
        "    lmax = ax.axvline(xmax, color=\"red\", linestyle=\":\", linewidth=1.5)\n",
        "    lines.extend([lmin, lmax])\n",
        "    labels.extend([\"Min (0%)\", \"Max (100%)\"])\n",
        "\n",
        "    # --- finalize ---\n",
        "    ax.set_title(f\"{title} (removed={int(rem.sum())})\")\n",
        "    ax.set_xlabel(\"log10 value\")\n",
        "    ax.set_ylabel(\"count\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.legend(lines, labels, fontsize=7, loc=\"upper right\", framealpha=0.9)\n",
        "\n",
        "\n",
        "# --- A) PRE: visualize continuous corrs + FP variance on raw data ---\n",
        "cont_cols_pre, fp_cols_pre = split_feature_blocks(enzy_data)\n",
        "plot_corr_heatmap(enzy_data, cont_cols_pre, \"Continuous descriptors (pre-pruning)\", \"cont_corr_pre.png\")\n",
        "if fp_cols_pre:\n",
        "    # pre-filter plot: pretend all fp bits are kept\n",
        "    plot_fp_variance_and_kept(\n",
        "        fps_df=enzy_data[fp_cols_pre],\n",
        "        kept_mask=np.ones(len(fp_cols_pre), dtype=bool),\n",
        "        title_prefix=\"Fingerprint (pre-filter)\",\n",
        "        var_threshold=FP_VAR_THRESHOLD,\n",
        "        fname_prefix=\"fp_pre\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- B) PRE: OUTLIER VISUALS on raw data (before dropping) ---\n",
        "masks_pre, meta_pre = compute_outlier_masks_with_meta(enzy_data, fence=1.5, contamination=0.01)\n",
        "plot_outlier_histograms(enzy_data, masks_pre, meta_pre, fname_prefix=\"outlier_hists_pre\")\n",
        "plot_kcat_km_scatter_with_outliers(enzy_data, masks_pre, fname=\"scatter_kcat_km_outliers_pre.png\")\n",
        "plot_iso_score_hist(meta_pre, masks_pre, fname=\"iso_scores_pre.png\")\n",
        "plot_outliers_pie(masks_pre, fname=\"outliers_pie_pre.png\")\n",
        "\n",
        "\n",
        "plot_kcat_km_scatter_by_reason(\n",
        "    enzy_data,          # or enzy_data_clean\n",
        "    masks_pre,          # or masks_post\n",
        "    sample=80000,\n",
        "    fname=\"scatter_kcat_km_by_reason.png\",\n",
        "    priority=(\"domain\",\"iso\",\"kinetics\")  # domain > kinetics > iso > regular\n",
        ")\n",
        "\n",
        "# build masks using the SAME settings as apply_outlier_strategy\n",
        "drop_masks = compute_sequential_drop_masks(\n",
        "    enzy_data,\n",
        "    fence=1.5,\n",
        "    contamination=0.01,\n",
        "    per_group=\"enzyme\",\n",
        ")\n",
        "\n",
        "# show only the dropped points; hide regulars entirely\n",
        "plot_kcat_km_scatter_drops(\n",
        "    enzy_data,\n",
        "    drop_masks,\n",
        "    # fname=\"scatter_kcat_km_dropped_only.png\",\n",
        "    show_regular=True,       # <- hide regular\n",
        "    # sample_regular=0          # <- ensure none are plotted\n",
        "    # sample_regular=10000\n",
        ")\n",
        "\n",
        "\n",
        "# --- C) Apply your outlier strategy to actually drop rows ---\n",
        "enzy_data_clean = apply_outlier_strategy(enzy_data)\n",
        "\n",
        "# --- D) Run feature preparation on the CLEANED data ---\n",
        "X, y, meta = prepare_features(enzy_data_clean)\n",
        "\n",
        "# --- E) POST: correlation + FP kept/dropped + PCA scree on CLEANED data ---\n",
        "plot_corr_heatmap(enzy_data_clean, meta[\"cont_cols_kept\"], \"Continuous descriptors (post-pruning)\", \"cont_corr_post.png\")\n",
        "\n",
        "if meta[\"fp_cols_in\"] and meta[\"fp_var_selector\"] is not None:\n",
        "    vt = meta[\"fp_var_selector\"]\n",
        "    kept_mask = vt.get_support()\n",
        "    plot_fp_variance_and_kept(\n",
        "        fps_df=enzy_data_clean[meta[\"fp_cols_in\"]],\n",
        "        kept_mask=kept_mask,\n",
        "        title_prefix=\"Fingerprint (post-filter)\",\n",
        "        var_threshold=FP_VAR_THRESHOLD,\n",
        "        fname_prefix=\"fp_post\"\n",
        "    )\n",
        "\n",
        "\n",
        "plot_pca_explained_variance(meta[\"fp_pca\"])\n",
        "\n",
        "# --- F) POST: OUTLIER VISUALS on cleaned data (sanity check) ---\n",
        "masks_post, meta_post = compute_outlier_masks_with_meta(enzy_data_clean, fence=1.5, contamination=0.01)\n",
        "plot_outlier_histograms(enzy_data_clean, masks_post, meta_post, fname_prefix=\"outlier_hists_post\")\n",
        "plot_kcat_km_scatter_with_outliers(enzy_data_clean, masks_post, fname=\"scatter_kcat_km_outliers_post.png\")\n",
        "plot_iso_score_hist(meta_post, masks_post, fname=\"iso_scores_post.png\")\n",
        "plot_outliers_pie(masks_post, fname=\"outliers_pie_post.png\")\n",
        "plot_umap_fingerprints(enzy_data_clean)\n",
        "\n",
        "\n",
        "print(\"\\n=== Done ===\")\n",
        "print(\"Figures saved to:\", os.path.abspath(FIG_DIR))\n",
        "print(\"Final X shape:\", X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== 所有列名 (enzy_data_clean) ===\")\n",
        "for col in enzy_data_clean.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "找到dataset是线性还是非线性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特征工程 & 机器学习\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# --- 新增：用于建模和评估的库 ---\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ridge (图 1)：线性模型无法处理数据，预测结果和真实值之间没有相关性。\n",
        "\n",
        "Random Forest (图 2)：非线性模型成功地捕捉到了数据中的复杂关系，预测值和真实值在 log 尺度上高度相关。\n",
        "\n",
        "需要使用log尺度标准，不可以转换回原始值，误差被指数型放大"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 单元格 5: 建模辅助函数 (新)\n",
        "# (定义绘图和特征名称的辅助工具)\n",
        "# ============================================\n",
        "\n",
        "def plot_parity_LOG_METRICS(y_true_orig, y_pred_orig, title, target_name_for_plot):\n",
        "    \"\"\"\n",
        "    绘制一个标准的一致性图 (Predicted vs. Actual)\n",
        "    y_true_orig 和 y_pred_orig 必须在 *原始尺度*\n",
        "    此版本将在图上显示 *Log 尺度* 的指标\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. 转换为 log10 尺度\n",
        "    # 我们添加一个极小值 1e-30 来避免 log(0) 错误\n",
        "    y_true_log = np.log10(y_true_orig + 1e-30)\n",
        "    y_pred_log = np.log10(y_pred_orig + 1e-30)\n",
        "    \n",
        "    # 2. **在 Log 尺度上计算指标**\n",
        "    r2_log = r2_score(y_true_log, y_pred_log)\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
        "\n",
        "    # --- 绘图代码 ---\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    # 找到稳健的绘图范围\n",
        "    min_val = np.percentile(np.concatenate([y_true_log, y_pred_log]), 1) * 1.1\n",
        "    max_val = np.percentile(np.concatenate([y_true_log, y_pred_log]), 99) * 1.1\n",
        "    if not np.isfinite(min_val): min_val = -10\n",
        "    if not np.isfinite(max_val): max_val = 10\n",
        "        \n",
        "    plt.scatter(y_true_log, y_pred_log, alpha=0.1, s=10, label=\"Data points\")\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label=\"Ideal (y=x)\")\n",
        "    \n",
        "    # 3. **使用 Log 尺度指标更新文本**\n",
        "    text_content = f\"$R^2$ (log) = {r2_log:.3f}\\nRMSE (log) = {rmse_log:.3f}\"\n",
        "    plt.text(min_val + (max_val - min_val) * 0.05, \n",
        "             max_val - (max_val - min_val) * 0.15, \n",
        "             text_content, \n",
        "             fontsize=12, \n",
        "             bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    plt.xlabel(f\"Actual log10({target_name_for_plot})\")\n",
        "    plt.ylabel(f\"Predicted log10({target_name_for_plot})\")\n",
        "    plt.title(f\"Parity Plot: {title}\\n(Target: {target_name_for_plot})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.axis('square')\n",
        "    plt.xlim(min_val, max_val)\n",
        "    plt.ylim(min_val, max_val)\n",
        "    \n",
        "    fname = f\"parity_plot_{target_name_for_plot}_{title.lower().replace(' ', '_')}.png\"\n",
        "    out = os.path.join(FIG_DIR, fname)\n",
        "    plt.savefig(out, dpi=200)\n",
        "    print(f\"[saved] {out}\")\n",
        "    plt.show() # 立即显示\n",
        "\n",
        "# --- 从 'meta' 字典中获取最终的特征名称 ---\n",
        "# ( 单元格 4 已经定义了 'meta' )\n",
        "all_feature_names_in_model_X = meta['cont_cols_kept'] + meta['fp_cols_kept']\n",
        "print(f\"已定义绘图函数。准备开始建模，总共使用 {len(all_feature_names_in_model_X)} 个特征。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 单元格 6: 自动化建模循环 (新)\n",
        "# ============================================\n",
        "\n",
        "# X 和 y 字典已在 单元格 4 中被全局定义\n",
        "# X 是 (n_samples, n_features) 数组\n",
        "# y 是 {'kcat_value': array, 'km_value': array, 'kcat_km': None}\n",
        "\n",
        "ALL_TARGETS_TO_MODEL = ['kcat_value', 'km_value', 'kcat_km']\n",
        "\n",
        "# 这是一个字典, 用来存储所有模型的评估结果, 以便最后比较\n",
        "results_summary = {}\n",
        "\n",
        "for target_name in ALL_TARGETS_TO_MODEL:\n",
        "    print(f\"\\n\\n========================================================\")\n",
        "    print(f\"=== [开始] 建模流程: 目标 = {target_name} ===\")\n",
        "    print(f\"========================================================\\n\")\n",
        "\n",
        "    y_raw = None\n",
        "    \n",
        "    # --- 1. 动态获取/计算 y_raw ---\n",
        "    # 这是此解决方案的关键步骤\n",
        "    \n",
        "    if target_name == 'kcat_km':\n",
        "        print(\"--- 步骤 1: 动态计算 'kcat_km' ---\")\n",
        "        if y['kcat_value'] is not None and y['km_value'] is not None:\n",
        "            # 从 单元格 4 的输出中获取数组\n",
        "            y_kcat = y['kcat_value']\n",
        "            y_km = y['km_value']\n",
        "            \n",
        "            # 安全地计算比率 (忽略除以零或无效值)\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                y_raw = y_kcat / y_km\n",
        "            print(f\"成功从 y['kcat_value'] / y['km_value'] 动态计算 'kcat_km'。\")\n",
        "        else:\n",
        "            print(f\"错误: 无法计算 'kcat_km'，因为 y['kcat_value'] 或 y['km_value'] 为 None。\")\n",
        "            print(\"请确保 单元格 4 成功运行并找到了 kcat_value 和 km_value。跳过此目标。\")\n",
        "            continue\n",
        "    \n",
        "    else:\n",
        "        # 对于 'kcat_value' 和 'km_value'，直接从 y 字典中获取\n",
        "        if y[target_name] is not None:\n",
        "            y_raw = y[target_name]\n",
        "            print(f\"--- 步骤 1: 成功从 y['{target_name}'] 获取数据 ---\")\n",
        "        else:\n",
        "            print(f\"错误: 目标 '{target_name}' 在 y 字典中未找到 (值为 None)。跳过此目标。\")\n",
        "            continue\n",
        "            \n",
        "    # --- 2. 过滤无效的 y 值 ---\n",
        "    # 这一步现在也会自动过滤掉 'kcat_km' 计算中产生的 NaN 或 inf\n",
        "    valid_mask = (~np.isnan(y_raw)) & (y_raw > 0) & (np.isfinite(y_raw))\n",
        "    X_final = X[valid_mask]\n",
        "    y_final = y_raw[valid_mask]\n",
        "\n",
        "    if len(y_final) == 0:\n",
        "        print(f\"目标 '{target_name}' 没有有效的 ( > 0 且有限) y 值。跳过此目标。\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"过滤无效y值后 X_final 形状: {X_final.shape}\")\n",
        "    print(f\"过滤无效y值后 y_final 形状: {y_final.shape}\")\n",
        "\n",
        "    # --- 3. Log-Transform Y ---\n",
        "    y_log = np.log10(y_final)\n",
        "\n",
        "    # --- 4. 划分训练集和测试集 ---\n",
        "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
        "        X_final, y_log, test_size=0.2, random_state=RANDOM_STATE\n",
        "    )\n",
        "    y_test_orig = 10**y_test_log # 转换回原始尺度用于最终评估\n",
        "    print(f\"数据划分为 {len(y_train_log)} 训练 / {len(y_test_orig)} 测试 样本。\")\n",
        "\n",
        "    # --- 5. 训练和评估 Ridge ---\n",
        "    print(\"\\n--- 步骤 2: 训练 Ridge (线性) ---\")\n",
        "    ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
        "    ridge_model.fit(X_train, y_train_log)\n",
        "    y_pred_log_ridge = ridge_model.predict(X_test)\n",
        "    y_pred_orig_ridge = 10**y_pred_log_ridge\n",
        "    \n",
        "    # Log 尺度指标\n",
        "    r2_log_ridge = r2_score(y_test_log, y_pred_log_ridge)\n",
        "    rmse_log_ridge = np.sqrt(mean_squared_error(y_test_log, y_pred_log_ridge))\n",
        "    # 原始尺度指标\n",
        "    r2_orig_ridge = r2_score(y_test_orig, y_pred_orig_ridge)\n",
        "    \n",
        "    print(f\"  R^2 (log): {r2_log_ridge:<7.4f} | RMSE (log): {rmse_log_ridge:<7.4f}\")\n",
        "    print(f\"  R^2 (orig): {r2_orig_ridge:<7.4f}\")\n",
        "\n",
        "    # --- 6. 训练和评估 Random Forest ---\n",
        "    print(\"\\n--- 步骤 3: 训练 Random Forest (非线性) ---\")\n",
        "    print(\"(正在训练...)\")\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1, oob_score=True\n",
        "    )\n",
        "    rf_model.fit(X_train, y_train_log)\n",
        "    y_pred_log_rf = rf_model.predict(X_test)\n",
        "    y_pred_orig_rf = 10**y_pred_log_rf\n",
        "\n",
        "    # Log 尺度指标\n",
        "    r2_log_rf = r2_score(y_test_log, y_pred_log_rf)\n",
        "    rmse_log_rf = np.sqrt(mean_squared_error(y_test_log, y_pred_log_rf))\n",
        "    # 原始尺度指标\n",
        "    r2_orig_rf = r2_score(y_test_orig, y_pred_orig_rf)\n",
        "    \n",
        "    print(f\"  OOB R^2 (log): {rf_model.oob_score_:.4f}\")\n",
        "    print(f\"  R^2 (log): {r2_log_rf:<7.4f} | RMSE (log): {rmse_log_rf:<7.4f}\")\n",
        "    print(f\"  R^2 (orig): {r2_orig_rf:<7.4f}\")\n",
        "    \n",
        "    # --- 7. 存储结果 ---\n",
        "    results_summary[target_name] = {\n",
        "        \"Ridge\": {\"R2_log\": r2_log_ridge, \"RMSE_log\": rmse_log_ridge, \"R2_orig\": r2_orig_ridge},\n",
        "        \"RF\": {\"R2_log\": r2_log_rf, \"RMSE_log\": rmse_log_rf, \"R2_orig\": r2_orig_rf}\n",
        "    }\n",
        "\n",
        "    # --- 8. 绘制 Parity Plots (使用单元格 5 中定义的函数) ---\n",
        "    print(\"\\n--- 步骤 4: 绘制 Parity Plots (显示 Log 指标) ---\")\n",
        "    plot_parity_LOG_METRICS(y_test_orig, y_pred_orig_ridge, \"Ridge Regression\", target_name)\n",
        "    plot_parity_LOG_METRICS(y_test_orig, y_pred_orig_rf, \"Random Forest\", target_name)\n",
        "\n",
        "    # --- 9. 绘制特征重要性 ---\n",
        "    print(\"\\n--- 步骤 5: 绘制特征重要性 (来自 RF) ---\")\n",
        "    importances = rf_model.feature_importances_\n",
        "    feat_imp_df = pd.DataFrame({\n",
        "        'feature': all_feature_names_in_model_X,\n",
        "        'importance': importances\n",
        "    }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "    print(f\"Top 5 most important features for {target_name}:\")\n",
        "    print(feat_imp_df.head(5).to_string())\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(\n",
        "        x='importance', y='feature', \n",
        "        data=feat_imp_df.head(20), \n",
        "        palette='viridis'\n",
        "    )\n",
        "    plt.title(f\"Top 20 Feature Importances (RF) for log10({target_name})\")\n",
        "    plt.xlabel(\"Importance (Gini reduction)\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out = os.path.join(FIG_DIR, f\"feature_importance_top20_{target_name}.png\")\n",
        "    plt.savefig(out, dpi=200)\n",
        "    print(f\"\\n[saved] {out}\")\n",
        "    plt.show() # 立即显示\n",
        "    \n",
        "print(\"\\n\\n=== [完成] 所有目标建模完毕 ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 单元格 7: 最终结果摘要 (新)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n========================================================\")\n",
        "print(f\"=== [最终摘要] 自动化建模流程 ===\")\n",
        "print(f\"========================================================\\n\")\n",
        "print(\"比较所有模型在 Log 尺度上的 R-squared (R2_log)\")\n",
        "print(\"R2_log 是评估此类数据的最佳指标\\n\")\n",
        "\n",
        "# 检查 results_summary 是否为空\n",
        "if not results_summary:\n",
        "    print(\"没有模型被成功训练，无法生成摘要。\")\n",
        "else:\n",
        "    # 格式化 DataFrame 以便清晰打印\n",
        "    summary_data = []\n",
        "    for target, models in results_summary.items():\n",
        "        for model_name, metrics in models.items():\n",
        "            summary_data.append({\n",
        "                \"Target\": target,\n",
        "                \"Model\": model_name,\n",
        "                \"R2 (log)\": metrics.get('R2_log', np.nan),\n",
        "                \"RMSE (log)\": metrics.get('RMSE_log', np.nan),\n",
        "                \"R2 (orig)\": metrics.get('R2_orig', np.nan),\n",
        "            })\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    \n",
        "    # 使用 Markdown 格式打印，非常清晰\n",
        "    print(summary_df.to_markdown(index=False, floatfmt=\".4f\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
